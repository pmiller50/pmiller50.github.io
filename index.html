<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Paul Miller - Data Scientist</title>
  <meta content="Paul Miller Portfolio" name="description">
  <meta content="Paul,Miller,Portfolio,resume,python,data,scientist,modeling,models,analysis,SQL,pandas,numpy,nlp," name="keywords">
  <meta name="author" content="Paul Miller">

  <!-- Favicons -->
  <link href="assets/img/favicon.ico" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: iPortfolio - v3.0.1
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

  <!-- ======= Header ======= -->
  <header id="header">
    <div class="d-flex flex-column">

      <div class="profile">
        <img src="assets/img/head_shot.png" alt="Photo" class="img-fluid rounded-circle">
        <h1 class="text-light"><a href="index.html">Paul Miller</a></h1>
        <div class="social-links mt-3 text-center">
          <a href="mailto:pdmill@gmail.com" class="envelope"><i class="bx bx-envelope"></i></a>
          <a href="https://github.com/pmiller50" class="git"><i class="bx bxl-git"></i></a>
          <a href="https://www.linkedin.com/in/pdmill/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
        </div>
      </div>

      <nav id="navbar" class="nav-menu navbar">
        <ul>
          <li><a href="#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
          <li><a href="#project_1" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>TED Talk Text Generator</span></a></li>
          <li><a href="#project_2" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Reddit Natural<br>Language Processing</span></a></li>
          <li><a href="#project_3" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Mass Protests</span></a></li>
          <li><a href="#project_4" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Ames, IA Housing</span></a></li>
        </ul>
      </nav><!-- .nav-menu -->
    </div>
  </header><!-- End Header -->

  <main id="main">

    <!-- ======= About Section ======= -->
    <section id="about" class="contact">
      <div class="container">

        <div class="section-title">
          <h2>About</h2>
          <p>I am a data scientist who can provide a fresh perspective and clear vision to help explain complex data.</p>
        </div>

        <div class="row">
          <div class="col-lg-10 pt-4 pt-lg-0 content">
            <p class="font-italic">
              I just completed a 3 month data science boot camp offered by <a href="https://generalassemb.ly/education/data-science-immersive-remote">General Assembly</a>,
               which gave me a solid foundation of Python, statistics, SQL, and various types of modeling, such as regression, sentiment analysis and neural networks.</p>
              <p>
              I worked for IBM for about 10 years in an application development role. More recently, I began getting involved with
               sports analytics and sports betting. Now I'd like to widen my view to look at applying data science to other fields.
            </p>
<p class="font-italic">I am currently based in Las Vegas, NV but am open to working remotely or relocating.
          </div>

      </div>

      </div>

       <div class="col-lg-5 d-flex align-items-stretch">
            <div class="info">
              <div class="email">
                <i class="bi bi-envelope"></i>
                <h4>Email:</h4>
                <p><a href="mailto:pdmill@gmail.com"</h>pdmill@gmail.com</p>
              </div>

              <div class="phone">
                <i class="bi bi-phone"></i>
                <h4>Call:</h4>
                <p>919-672-3099</p>
              </div>

              <div class="phone">
                <i class="bx bxl-linkedin"></i>
                <h4>LinkedIn:</h4>
                <p><a href="https://www.linkedin.com/in/pdmill/">linkedin.com/in/pdmill/</a></p>
              </div>

            </div>
      </div>
    </section><!-- End Contact Section -->

    <!-- ======= Project 1 Section ======= -->
    <section id="project_1" class="resume">
      <div class="container">

        <div class="section-title">
          <h2>Project: TED Talk Text Generator</h2>
        </div>

        <div class="row">
          <div class="col-lg-10 pt-4 pt-lg-0 content">
            <p>Interest in Artificial Intelligence and text generation spiked in May of 2020, when Open AI Labs released a new version of their deep
              learning model called GPT-3. GPT-3 is a highly trained text generation engine built by reading 45 TB of data from public web sites,
              and electronic books and other sources. GPT-3 uses 175 billion parameters, and produces an amazingly high level of quality text.</p>

            <br>
            <p>This project attempts to pursue if and how a simple text generator could be built, and whether the quality of the output text could pass as something legible and understood by humans.</p>
          </div>

      </div>

<br>
        <div class="row">
          <div class="resume-item col-lg-10">
          <h5>Data</h5>

<p>All text generation models require a group of text, also called a corpus. This project used the BeautifulSoup Python library to scan
  the <a href="https://www.ted.com/talks">TED.com library browse page</a> to retrieve a list of URLs for all talks in English.

This list of all talks can then be used to scrape the transcript text for each talk.</p>

<ul>
  <li>Scraping TED.com created a corpus of 4,384 talks.</li>
  <li>Using the entire corpus overloaded the Google Colab environement, so a smaller corpus of just the top 20 most viewed talks was used.</li>
  <li>Top 20 talks transcripts consisted of approximately 270,000 characters.
</ul>

<h5>Modeling</h5>
<p>Text generation with machine learning is essentially a multiclass classification problem.</p>
<p>Classes are the labels assigned to the next character or word a model is predicting, also known as it's target variable (y).
<br>
For example, in the sentence "Welcome to my TED talk.", a model uses a sequence length of 8, would use the first 8 characters as its input variables, in order to predict the 9th character.
</p>
<img src="./assets/img/welcome_2.png" width="300" height="329" alt="character prediction">
<br><br>
<p>Various configurations of Long Short Term Memory recurrent neural networks were used to create models to predict (generate) characters, after
  being trained on the corpus.

<h5>Output</h5>
<p>Using a starting point of a random 40 character sequence in the corpus, the model was used to predict the next 600 characters.</p>
<p>The output text was fairly accurate for the early portions of text, but then would become unreadable.

(The original 40 character sequence in <b><span style="color:red">red</span></b>.)</p>

<blockquote>
<b><span style="color:red">re can do it faster. Lowcost providers c</span></b>an do it cheaper. So what really matters are the more rightbrained creative conceptual kinds of abilities. This is no surprise though if you look at the insights of contemporary psychology. It turns out that we cant even be in a group of people without instingtively mindlrming something tr musi but one in this woul world. I tas 190 million people didd from this kind of worthiness. Fo earter. When youre io the lowert resson there because when they cid that.with the pame thing. The sewr of shis ptise of this lide of madies and shose are amrays that farcer. When you choose to view stress in this
</blockquote>

<h5>Conclusions</h5>
<p>In general, it seems as if the models would predict the next characters and sometimes words, but it begs the question:
  Is this text generation? At times the text generated was simply a repeat of a different, randomly selected portion of the corpus.
   It's possible that a larger corpus would provide a larger vocabulary, and more randomness,
   hence presenting a more interesting output.</p>

<em>More information and full readme found in the <a href="https://github.com/pmiller50/ted_talk_gen">Github repository</a>.</em>
<br><br>

        </div>

      </div>
    </section><!-- End Project 1 Section -->

    <!-- ======= Project 2 Section ======= -->
    <section id="project_2" class="resume">
      <div class="container">
        <div class="section-title">
          <h2>Reddit Natural Language Processing</h2>
        </div>

          <div class="row">
               <div class="resume-item col-lg-10">
          <p>This project aimed to build classification models and perform sentiment analysis for two separate Reddit categories.</p>

<p>Consider the two activities:
<ul>
  <li>Cooking</li>
  <li>Running</li>
</ul>
One could argue that cooking, for the most part, pertains to family gatherings, holidays, birthdays and other group social occasions.</p>
<p>In contrast, running is primarily a solitary activity.</p>

<p>Is being alone necessarily a bad thing? In regards to text classification, would we expect to see more negative sentiment classifications
   in text that is more related to independently performed activities?</p>

   <p>The general consensus of the <a href="https://www.psychalive.org/being-alone/">medical field</a> seems to agree, that too much alone
      time is not healthy. So the expection is that the running Reddit posts would have a higher number of words such as
      "alone", "lonely", "lone", etc. This would also lead to a more negative sentiment score for the running posts as compared
      to the culinary posts.

  </p>

<h5>Data</h5>
<p>Data was collected from reddit.com using the <a href="https://pushshift.io/">Pushshift API</a> to gather over 8,000 posts for each of
  two subreddits.
  <ul>
    <li><a href="https://www.reddit.com/r/AskCulinary/">/AskCulinary</a></li>
    <li><a href="https://www.reddit.com/r/running/">/running</a></li>
  </ul>
</p>

<h5>Modeling</h5>
<p>The project is grouped into two logical divisions. One segment involved building classifiers, which would label a post into one of two
  subreddit categories.</p>
<p>The other segment of the project was work using SentimentIntensityAnalyzer, part of
  <a href="https://github.com/cjhutto/vaderSentiment">nltk.sentiment.vader</a>.
  The Natural Language Toolkit (NLTK), provides several tools to parse and analyze text data.</p>

<h5>Output</h5>
<p>Every post in both the Ask Culinary and Running subreddits is assigned a compound sentiment score, created by the SentimentIntensityAnalyzer.
These scores are a normalized version of their negative, neutral, and positive rankings, and range from -1 to +1.</p>
<p>The following shows the relative positivity or negativity of all posts in both subreddits.
  <br>  <br>
<img src="./assets/img/all_posts_scores.png" width="624" height="411" alt="Sentiment scores for all posts">
</p>


<p>In the 8,000 Ask Culinary records, the word "alone" appears 40 times, and the word lonely appears twice.
  <br><br>
  In the 8,000 Running records, the word "alone" appears 125 times, along with lesser occurrences of the words: lonely, lone, lonelier, and loneliness.
</p>

<h5>Conclusions</h5>
<p>Using the outputs from the VADER SentimentIntensityAnalyzer module, the Running posts do in fact have a more negative
   overall sentiment. It is still too early to make any large-scale statements about the human behavior or personal well-being
    of the users of the Ask Culinary group versus the Running users, but the text collected from the Running subReddit does
    show a clear (albeit slight) negative bias.
  </p>

  <em>More information and full readme found in the <a href="https://github.com/pmiller50/Reddit_NLP">Github repository</a>.</em>
  <br><br>
            <div>
          </div>

      </div>
    </section><!-- End Project 2 Section -->

    <!-- ======= Project 3 Section ======= -->
    <section id="project_3" class="resume">
      <div class="container">
        <div class="section-title">
          <h2>Mass Protests</h2>
        </div>

          <div class="row">
               <div class="resume-item col-lg-10">
          <p>This is a group project completed during a <a href="https://generalassemb.ly/education/data-science-immersive-remote">General Assembly</a> boot camp, with the following team members:
<ul>
  <li><a href="https://www.linkedin.com/in/emily-naftalin/">Emily Naftalin</a></li>
  <li><a href="https://www.linkedin.com/in/hayden-tsutsui/">Hayden Tsutsui</a></li>
  <li><a href="https://www.linkedin.com/in/heatherjohansen/">Heather Johansen</a></li>
</ul>
</p>
<p>The <a href="https://massmobilization.github.io">Mass Mobilization Project</a> hosts data about citizen movements against governments.
  The original intent of the MM study was to inform foreign policy and understand the impact of mass mobilizations outside the United States.
  One main question to study was could we accurately predict a government's response to a given protest.
   </p>
<br>

<h5>Data</h5>
<p>The Mass Mobilization Project provided the bulk of the data set, which scraped information for over 17,000 protests across the world
   from the New York Times, Washington Post, Christian Science Monitor, and Times of London from 1990-2020.
A protest record required 50 or more participants to qualify.
<br><br>
Other sources that were added to supplement the Mass Mobilization data:
<ul>
  <li>Population data as provided by the <a href="https://population.un.org/wpp/Download/Standard/CSV/">United Nations</a></li>
  <li>A Prosperity Index rating as calculated by the <a href="https://www.prosperity.com/about/methodology">Legatum Institute</a></li>
</ul>
</p>

<h5>Modeling</h5>
<p>After cleaning data and encoding categorical variables, several different modeling techniques were employed:
<ul>
  <li>Multilabel classifier</li>
  <li>Seven different logistic regression models - one for each of the possible state responses</li>
  <li>Due to an imbalanced dataset (over 50% of state responses were "Ignore"), the data was filtered to only
    include a record which was not ignored, and modeled again</li>
  </ul>
</p>

<h5>Output</h5>
<p>Metrics for the models had various results, given each target variable (state response):
  <table>
  <thead>
  <tr>
  <th>Target</th>
  <th>cross val</th>
  <th>specificity</th>
  <th>precision</th>
  <th>recall</th>
  </tr>
  </thead>
  <tbody>
  <tr>
  <td>ignore</td>
  <td>0.787</td>
  <td>0.533</td>
  <td><strong>0.700</strong></td>
  <td><strong>0.894</strong></td>
  </tr>
  <tr>
  <td>crowd dispersal</td>
  <td>0.726</td>
  <td>0.316</td>
  <td><strong>0.727</strong></td>
  <td><strong>0.899</strong></td>
  </tr>
  <tr>
  <td>arrests</td>
  <td>0.619</td>
  <td>0.276</td>
  <td><strong>0.343</strong></td>
  <td><strong>0.861</strong></td>
  </tr>
  <tr>
  <td>accommodation</td>
  <td>0.726</td>
  <td>0.735</td>
  <td><strong>0.364</strong></td>
  <td><strong>0.581</strong></td>
  </tr>
  <tr>
  <td>killings</td>
  <td>0.779</td>
  <td><strong>0.648</strong></td>
  <td>0.224</td>
  <td><strong>0.797</strong></td>
  </tr>
  <tr>
  <td>beatings</td>
  <td>0.793</td>
  <td><strong>0.595</strong></td>
  <td>0.242</td>
  <td><strong>0.852</strong></td>
  </tr>
  <tr>
  <td>shootings</td>
  <td>0.683</td>
  <td><strong>0.584</strong></td>
  <td>0.180</td>
  <td><strong>0.726</strong></td>
  </tr>
  </tbody>
  </table>

When evaluating multiple targets, some of which are desirable and some not, metric selection can be complicated.
 In addition, the imbalanced classes in our data impacted some metrics more than others.

</p>

<h5>Conclusions</h5>
<p>Conclusions for the ignore target were relatively straight forward and expected. For instance, governments
   were more likely to ignore protests in countries with a higher prosperity index score. An interesting nuance
   in the findings is governments were less likely to ignore small protests and more likely to ignore larger protests.
  </p>

<img src="./assets/img/ignore_coefficients.png" width="712" height="285" alt="ignore coefficients">

<br><br>
  <em>More information and full readme found in the <a href="https://github.com/pmiller50/Mass-Protests">Github repository</a>.</em>
  <br><br>
            <div>
          </div>

      </div>
    </section><!-- End Project 3 Section -->

    <!-- ======= Project 4 Section ======= -->
    <section id="project_4" class="resume">
      <div class="container">

        <div class="section-title">
          <h2>Ames, IA Housing</h2>
        </div>

        <div class="row">
          <div class="col-lg-10 pt-4 pt-lg-0 content">
            <p>Interest in Artificial Intelligence and text generation spiked in May of 2020, when Open AI Labs released a new version of their deep
              learning model called GPT-3. GPT-3 is a highly trained text generation engine built by reading 45 TB of data from public web sites,
              and electronic books and other sources. GPT-3 uses 175 billion parameters, and produces an amazingly high level of quality text.</p>

            <br>
            <p>This project attempts to pursue if and how a simple text generator could be built, and whether the quality of the output text could pass as something legible and understood by humans.</p>
          </div>

      </div>

  <br>
        <div class="row">
          <div class="resume-item col-lg-10">
          <h5>Data</h5>

  <p>All text generation models require a group of text, also called a corpus. This project used the BeautifulSoup Python library to scan
  the <a href="https://www.ted.com/talks">TED.com library browse page</a> to retrieve a list of URLs for all talks in English.

  This list of all talks can then be used to scrape the transcript text for each talk.</p>

  <ul>
  <li>Scraping TED.com created a corpus of 4,384 talks.</li>
  <li>Using the entire corpus overloaded the Google Colab environement, so a smaller corpus of just the top 20 most viewed talks was used.</li>
  <li>Top 20 talks transcripts consisted of approximately 270,000 characters.
  </ul>

  <h5>Modeling</h5>
  <p>Text generation with machine learning is essentially a multiclass classification problem.</p>
  <p>Classes are the labels assigned to the next character or word a model is predicting, also known as it's target variable (y).
  <br>
  For example, in the sentence "Welcome to my TED talk.", a model uses a sequence length of 8, would use the first 8 characters as its input variables, in order to predict the 9th character.
  </p>
  <img src="./assets/img/welcome_2.png" width="300" height="329" alt="character prediction">
  <br><br>
  <p>Various configurations of Long Short Term Memory recurrent neural networks were used to create models to predict (generate) characters, after
  being trained on the corpus.

  <h5>Output</h5>
  <p>Using a starting point of a random 40 character sequence in the corpus, the model was used to predict the next 600 characters.</p>
  <p>The output text was fairly accurate for the early portions of text, but then would become unreadable.

  (The original 40 character sequence in <b><span style="color:red">red</span></b>.)</p>

  <blockquote>
  <b><span style="color:red">re can do it faster. Lowcost providers c</span></b>an do it cheaper. So what really matters are the more rightbrained creative conceptual kinds of abilities. This is no surprise though if you look at the insights of contemporary psychology. It turns out that we cant even be in a group of people without instingtively mindlrming something tr musi but one in this woul world. I tas 190 million people didd from this kind of worthiness. Fo earter. When youre io the lowert resson there because when they cid that.with the pame thing. The sewr of shis ptise of this lide of madies and shose are amrays that farcer. When you choose to view stress in this
  </blockquote>

  <h5>Conclusions</h5>
  <p>In general, it seems as if the models would predict the next characters and sometimes words, but it begs the question:
  Is this text generation? At times the text generated was simply a repeat of a different, randomly selected portion of the corpus.
   It's possible that a larger corpus would provide a larger vocabulary, and more randomness,
   hence presenting a more interesting output.</p>

  <em>More information and full readme found in the <a href="https://github.com/pmiller50/ted_talk_gen">Github repository</a>.</em>
  <br><br>

        </div>

      </div>
    </section><!-- End Project 4 Section -->




  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">
    <div class="container">
      <div class="copyright">
        &copy; Copyright <strong><span>iPortfolio</span></strong>
      </div>
      <div class="credits">
        <!-- All the links in the footer should remain intact. -->
        <!-- You can delete the links only if you purchased the pro version. -->
        <!-- Licensing information: https://bootstrapmade.com/license/ -->
        <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/ -->
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
      </div>
    </div>
  </footer><!-- End  Footer -->

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/purecounter/purecounter.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/typed.js/typed.min.js"></script>
  <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
